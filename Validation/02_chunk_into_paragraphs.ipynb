{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lzma\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the .tsv.xz file\n",
    "compressed_file_path = \"data/00_downloads/dataset.tsv.xz\"\n",
    "\n",
    "# Extract and read the .tsv.xz file directly into a pandas DataFrame\n",
    "with lzma.open(compressed_file_path) as file:\n",
    "    df = pd.read_csv(file, delimiter='\\t')\n",
    "\n",
    "df_unique = df.drop_duplicates(subset=['content_id'], keep='first')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract the relevant columns\n",
    "text_column = df_unique['content']\n",
    "id_column = df_unique['id']\n",
    "medium_code_column = df_unique['medium_code']\n",
    "doctype_column = df_unique['doctype']\n",
    "language_column = df_unique['language']\n",
    "head_column = df_unique['head']\n",
    "\n",
    "# Step 2: Function to extract paragraphs from the text\n",
    "def extract_paragraphs(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    paragraphs = soup.find_all('p')\n",
    "    return [p.get_text() for p in paragraphs]\n",
    "\n",
    "# Step 3: Apply the function to each row and create chunks\n",
    "df_unique['text'] = text_column.apply(extract_paragraphs)\n",
    "\n",
    "# Step 4: Flatten the list of paragraphs and add the original 'id' as a column\n",
    "all_paragraphs = []\n",
    "ids = []\n",
    "medium_codes = []\n",
    "doctypes = []\n",
    "languages = []\n",
    "heads = []\n",
    "\n",
    "for original_id, medium_code, doctype, language, head, paragraph_list in zip(id_column,\n",
    "                                                                             medium_code_column,\n",
    "                                                                             doctype_column,\n",
    "                                                                             language_column,\n",
    "                                                                             head_column,\n",
    "                                                                             df_unique['text']):\n",
    "    for paragraph in paragraph_list:\n",
    "        all_paragraphs.append(paragraph)\n",
    "        ids.append(original_id)  # Use the original 'id'\n",
    "        medium_codes.append(medium_code)\n",
    "        doctypes.append(doctype)\n",
    "        languages.append(language)\n",
    "        heads.append(head)\n",
    "\n",
    "# Step 5: Create a DataFrame for the Paragraphs Dataset\n",
    "df_paragraphs = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'text': all_paragraphs,\n",
    "    'medium_code': medium_codes,\n",
    "    'doctype': doctypes,\n",
    "    'language': languages,\n",
    "    'head': heads\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Paragraphs that don't include one of our keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of keywords\n",
    "keywords = [\n",
    "    \"Lebensmittelsystem\",\n",
    "    \"Nachhaltigkeit\",\n",
    "    \"Landwirtschaft\",\n",
    "    \"Klimawandel\",\n",
    "    \"Agrarpolitik\",\n",
    "    \"Biodiversität\",\n",
    "    \"Lebensmittelsicherheit\",\n",
    "    \"Regionalität\",\n",
    "    \"Agroökologie\",\n",
    "    \"Pestizidfreiheit\",\n",
    "    \"Ernährung\",\n",
    "    \"Treibhausgas\",\n",
    "    \"Biolandbau\",\n",
    "    \"Importabhängigkeit\",\n",
    "    \"Ernährungspolitik\",\n",
    "    \"Système alimentaire\",\n",
    "    \"Durabilité\",\n",
    "    \"Agriculture\",\n",
    "    \"Climat\",\n",
    "    \"Politique agricole\",\n",
    "    \"Biodiversité\",\n",
    "    \"Sécurité alimentaire\",\n",
    "    \"Localité\",\n",
    "    \"Agroécologie\",\n",
    "    \"pesticides\",\n",
    "    \"Alimentation\",\n",
    "    \"Gaz à effet de serre\",\n",
    "    \"Agriculture biologique\",\n",
    "    \"Importation\",\n",
    "    \"Politique alimentaire\",\n",
    "    \"Sistema alimentare\",\n",
    "    \"Sostenibilità\",\n",
    "    \"Agricoltura\",\n",
    "    \"Clima\",\n",
    "    \"Politica agricola\",\n",
    "    \"Biodiversità\",\n",
    "    \"Sicurezza alimentare\",\n",
    "    \"Località\",\n",
    "    \"Agroecologia\",\n",
    "    \"pesticidi\",\n",
    "    \"Alimentazione\",\n",
    "    \"Gas serra\",\n",
    "    \"Agricoltura biologica\",\n",
    "    \"Importazioni\",\n",
    "    \"Politica alimentare\"\n",
    "]\n",
    "\n",
    "# Convert the list of keywords to a Pandas Series\n",
    "keywords_series = pd.Series(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_paragraphs\n",
    "\n",
    "# Function to normalize and remove accents\n",
    "def normalize(text):\n",
    "    if isinstance(text, str):  # Ensure the value is a string\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        text = text.encode('ASCII', 'ignore').decode('utf-8')\n",
    "        return text.lower()\n",
    "    return ''  # Return an empty string for non-string values\n",
    "\n",
    "\n",
    "# Normalize and convert the 'text' column\n",
    "df['text_normalized'] = df['text'].apply(normalize)\n",
    "\n",
    "# Normalize the keywords\n",
    "normalized_keywords = [normalize(keyword) for keyword in keywords]\n",
    "\n",
    "# Create a regex pattern for the keywords\n",
    "pattern = '|'.join(re.escape(keyword) for keyword in normalized_keywords)\n",
    "\n",
    "# Create a boolean mask where any normalized keyword is found in the normalized 'text' column\n",
    "mask = df['text_normalized'].str.contains(pattern, case=False, na=False)\n",
    "\n",
    "# Apply the mask to filter the DataFrame\n",
    "filtered_df = df[mask]\n",
    "\n",
    "# Drop the normalized column (optional)\n",
    "filtered_df = filtered_df.drop(columns=['text_normalized'])\n",
    "\n",
    "# Create unique IDs for the filtered DataFrame\n",
    "filtered_df['unique_id'] = range(1, len(filtered_df) + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the paragraphs to a new CSV\n",
    "filtered_df.to_csv('data/01_text_data/paragraphs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dri_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
